{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Classifying-newswires:-a-multi-class-classification-example\" data-toc-modified-id=\"Classifying-newswires:-a-multi-class-classification-example-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Classifying newswires: a multi-class classification example</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-Reuters-dataset\" data-toc-modified-id=\"The-Reuters-dataset-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>The Reuters dataset</a></span></li><li><span><a href=\"#Preparing-the-data\" data-toc-modified-id=\"Preparing-the-data-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Preparing the data</a></span></li><li><span><a href=\"#Building-our-network\" data-toc-modified-id=\"Building-our-network-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Building our network</a></span></li><li><span><a href=\"#Validating-our-approach\" data-toc-modified-id=\"Validating-our-approach-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Validating our approach</a></span></li><li><span><a href=\"#Generating-predictions-on-new-data\" data-toc-modified-id=\"Generating-predictions-on-new-data-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Generating predictions on new data</a></span></li><li><span><a href=\"#A-different-way-to-handle-the-labels-and-the-loss\" data-toc-modified-id=\"A-different-way-to-handle-the-labels-and-the-loss-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>A different way to handle the labels and the loss</a></span></li><li><span><a href=\"#On-the-importance-of-having-sufficiently-large-intermediate-layers\" data-toc-modified-id=\"On-the-importance-of-having-sufficiently-large-intermediate-layers-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>On the importance of having sufficiently large intermediate layers</a></span></li><li><span><a href=\"#Further-experiments\" data-toc-modified-id=\"Further-experiments-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Further experiments</a></span></li><li><span><a href=\"#Wrapping-up\" data-toc-modified-id=\"Wrapping-up-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Wrapping up</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.1.3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5　新闻分类：多分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying newswires: a multi-class classification example\n",
    "\n",
    "This notebook contains the code samples found in Chapter 3, Section 5 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
    "\n",
    "----\n",
    "\n",
    "In the previous section we saw how to classify vector inputs into two mutually exclusive classes using a densely-connected neural network. \n",
    "But what happens when you have more than two classes? \n",
    "\n",
    "In this section, we will build a network to classify Reuters newswires into 46 different mutually-exclusive topics. Since we have many \n",
    "classes, this problem is an instance of \"multi-class classification\", and since each data point should be classified into only one \n",
    "category, the problem is more specifically an instance of \"single-label, multi-class classification\". If each data point could have \n",
    "belonged to multiple categories (in our case, topics) then we would be facing a \"multi-label, multi-class classification\" problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上一节中，我们介绍了如何用密集连接的神经网络将向量输入划分为两个互斥的类别。但如果类别不止两个，要怎么做？  \n",
    "\n",
    "本节你会构建一个网络，将路透社新闻划分为 46 个互斥的主题。因为有多个类别，所以\n",
    "这是多分类（multiclass classification）问题的一个例子。因为每个数据点只能划分到一个类别，\n",
    "所以更具体地说，这是单标签、多分类（single-label, multiclass classification）问题的一个例\n",
    "子。如果每个数据点可以划分到多个类别（主题），那它就是一个多标签、多分类（multilabel, \n",
    "multiclass classification）问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Reuters dataset\n",
    "\n",
    "\n",
    "We will be working with the _Reuters dataset_, a set of short newswires and their topics, published by Reuters in 1986. It's a very simple, \n",
    "widely used toy dataset for text classification. There are 46 different topics; some topics are more represented than others, but each \n",
    "topic has at least 10 examples in the training set.\n",
    "\n",
    "Like IMDB and MNIST, the Reuters dataset comes packaged as part of Keras. Let's take a look right away:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5.1　路透社数据集  \n",
    "本节使用路透社数据集，它包含许多短新闻及其对应的主题，由路透社在 1986 年发布。它\n",
    "是一个简单的、广泛使用的文本分类数据集。它包括 46 个不同的主题：某些主题的样本更多，\n",
    "但训练集中每个主题都有至少 10 个样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
      "2113536/2110848 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "?reuters.load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Like with the IMDB dataset, the argument `num_words=10000` restricts the data to the 10,000 most frequently occurring words found in the \n",
    "data.\n",
    "\n",
    "We have 8,982 training examples and 2,246 test examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与 IMDB 数据集一样，参数 num_words=10000 将数据限定为前 10 000 个最常出现的单词。\n",
    "我们有 8982 个训练样本和 2246 个测试样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8982"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2246"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the IMDB reviews, each example is a list of integers (word indices):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与 IMDB 评论一样，每个样本都是一个整数列表（表示单词索引）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 245,\n",
       " 273,\n",
       " 207,\n",
       " 156,\n",
       " 53,\n",
       " 74,\n",
       " 160,\n",
       " 26,\n",
       " 14,\n",
       " 46,\n",
       " 296,\n",
       " 26,\n",
       " 39,\n",
       " 74,\n",
       " 2979,\n",
       " 3554,\n",
       " 14,\n",
       " 46,\n",
       " 4689,\n",
       " 4329,\n",
       " 86,\n",
       " 61,\n",
       " 3499,\n",
       " 4795,\n",
       " 14,\n",
       " 61,\n",
       " 451,\n",
       " 4329,\n",
       " 17,\n",
       " 12]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[10]\n",
    "# 1 代表文章开始\n",
    "# 2 代表超出常用单词以外的表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you can decode it back to words, in case you are curious:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果好奇的话，你可以用下列代码将索引解码为单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters_word_index.json\n",
      "557056/550378 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "# Note that our indices were offset by 3\n",
    "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_newswire\n",
    "# 前面若干文本应该是人的名字，可通过num_words参数调大，可能就可以看到这几个字符了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label associated with an example is an integer between 0 and 45: a topic index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "样本对应的标签是一个 0~45 范围内的整数，即话题索引编号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  4,  3, ..., 25,  3, 25], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30979"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "We can vectorize the data with the exact same code as in our previous example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5.2　准备数据  \n",
    "你可以使用与上一个例子相同的代码将数据向量化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/3-5-多维.png' >\n",
    "实际上多维比一维需要多做几个这样的表格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(train_data) #将训练数据向量化\n",
    "# Our vectorized test data\n",
    "x_test = vectorize_sequences(test_data) #将测试数据向量化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To vectorize the labels, there are two possibilities: we could just cast the label list as an integer tensor, or we could use a \"one-hot\" \n",
    "encoding. One-hot encoding is a widely used format for categorical data, also called \"categorical encoding\". \n",
    "For a more detailed explanation of one-hot encoding, you can refer to Chapter 6, Section 1. \n",
    "In our case, one-hot encoding of our labels consists in embedding each label as an all-zero vector with a 1 in the place of the label index, e.g.:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将标签向量化有两种方法：你可以将标签列表转换为整数张量，或者使用 one-hot 编码。\n",
    "one-hot 编码是分类数据广泛使用的一种格式，也叫分类编码（categorical encoding）。6.1 节给出\n",
    "了 one-hot 编码的详细解释。在这个例子中，标签的 one-hot 编码就是将每个标签表示为全零向量，\n",
    "只有标签索引对应的元素为 1。其代码实现如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "\n",
    "# Our vectorized training labels\n",
    "one_hot_train_labels = to_one_hot(train_labels)\n",
    "# Our vectorized test labels\n",
    "one_hot_test_labels = to_one_hot(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看不太懂的时候在循环里加上几个print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 3)\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(1, 4)\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(2, 3)\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(3, 46)\n"
     ]
    }
   ],
   "source": [
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "        print((i,label))\n",
    "        print(results)\n",
    "    return results\n",
    "one_hot_train_labels=to_one_hot(train_labels[0:3])\n",
    "print(one_hot_train_labels)\n",
    "print(one_hot_train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is a built-in way to do this in Keras, which you have already seen in action in our MNIST example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，Keras 内置方法可以实现这个操作，你在 MNIST 例子中已经见过这种方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8982, 46)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our network\n",
    "\n",
    "\n",
    "This topic classification problem looks very similar to our previous movie review classification problem: in both cases, we are trying to \n",
    "classify short snippets of text. There is however a new constraint here: the number of output classes has gone from 2 to 46, i.e. the \n",
    "dimensionality of the output space is much larger. \n",
    "\n",
    "In a stack of `Dense` layers like what we were using, each layer can only access information present in the output of the previous layer. \n",
    "If one layer drops some information relevant to the classification problem, this information can never be recovered by later layers: each \n",
    "layer can potentially become an \"information bottleneck\". In our previous example, we were using 16-dimensional intermediate layers, but a \n",
    "16-dimensional space may be too limited to learn to separate 46 different classes: such small layers may act as information bottlenecks, \n",
    "permanently dropping relevant information.\n",
    "\n",
    "For this reason we will use larger layers. Let's go with 64 units:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5.3　构建网络  \n",
    "这个主题分类问题与前面的电影评论分类问题类似，两个例子都是试图对简短的文本片段\n",
    "进行分类。但这个问题有一个新的约束条件：输出类别的数量从 2 个变为 46 个。输出空间的维\n",
    "度要大得多。  \n",
    "\n",
    "对于前面用过的 Dense 层的堆叠，每层只能访问上一层输出的信息。如果某一层丢失了与\n",
    "分类问题相关的一些信息，那么这些信息无法被后面的层找回，也就是说，每一层都可能成为\n",
    "信息瓶颈。上一个例子使用了 16 维的中间层，但对这个例子来说 16 维空间可能太小了，无法\n",
    "学会区分 46 个不同的类别。这种维度较小的层可能成为信息瓶颈，永久地丢失相关信息。  \n",
    "\n",
    "出于这个原因，下面将使用维度更大的层，包含 64 个单元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax')) #最后一层再用softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 64)                640064    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 46)                2990      \n",
      "=================================================================\n",
      "Total params: 647,214\n",
      "Trainable params: 647,214\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "#(10000+1)*64 这里的+1是加上一个截距项\n",
    "#(64+1)*64\n",
    "#(64+1)*46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are two other things you should note about this architecture:\n",
    "\n",
    "* We are ending the network with a `Dense` layer of size 46. This means that for each input sample, our network will output a \n",
    "46-dimensional vector. Each entry in this vector (each dimension) will encode a different output class.\n",
    "* The last layer uses a `softmax` activation. You have already seen this pattern in the MNIST example. It means that the network will \n",
    "output a _probability distribution_ over the 46 different output classes, i.e. for every input sample, the network will produce a \n",
    "46-dimensional output vector where `output[i]` is the probability that the sample belongs to class `i`. The 46 scores will sum to 1.\n",
    "\n",
    "The best loss function to use in this case is `categorical_crossentropy`. It measures the distance between two probability distributions: \n",
    "in our case, between the probability distribution output by our network, and the true distribution of the labels. By minimizing the \n",
    "distance between these two distributions, we train our network to output something as close as possible to the true labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于这个架构还应该注意另外两点。  \n",
    "\n",
    "*  网络的最后一层是大小为 46 的 Dense 层。这意味着，对于每个输入样本，网络都会输\n",
    "出一个 46 维向量。这个向量的每个元素（即每个维度）代表不同的输出类别。\n",
    "*  最后一层使用了 softmax 激活。你在 MNIST 例子中见过这种用法。网络将输出在 46\n",
    "个不同输出类别上的概率分布——对于每一个输入样本，网络都会输出一个 46 维向量，\n",
    "其中 output[i] 是样本属于第 i 个类别的概率。46 个概率的总和为 1。  \n",
    "\n",
    "对于这个例子，最好的损失函数是 categorical_crossentropy（分类交叉熵）。它用于\n",
    "衡量两个概率分布之间的距离，这里两个概率分布分别是网络输出的概率分布和标签的真实分\n",
    "布。通过将这两个分布的距离最小化，训练网络可使输出结果尽可能接近真实标签。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compile：模型编译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy', #多元分类的交叉熵\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating our approach\n",
    "\n",
    "Let's set apart 1,000 samples in our training data to use as a validation set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5.4　验证你的方法\n",
    "我们在训练数据中留出 1000 个样本作为验证集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our network for 20 epochs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在开始训练网络，共 20 个轮次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 1s 31ms/step - loss: 3.2084 - accuracy: 0.3646 - val_loss: 1.7760 - val_accuracy: 0.6500\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.4995 - accuracy: 0.7086 - val_loss: 1.3269 - val_accuracy: 0.7200\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.0637 - accuracy: 0.7786 - val_loss: 1.1422 - val_accuracy: 0.7550\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.8279 - accuracy: 0.8251 - val_loss: 1.0233 - val_accuracy: 0.7850\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.6520 - accuracy: 0.8677 - val_loss: 0.9659 - val_accuracy: 0.7970\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.5234 - accuracy: 0.8967 - val_loss: 0.9285 - val_accuracy: 0.8120\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.4225 - accuracy: 0.9143 - val_loss: 0.9376 - val_accuracy: 0.7950\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.3347 - accuracy: 0.9322 - val_loss: 0.9035 - val_accuracy: 0.8140\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.2818 - accuracy: 0.9445 - val_loss: 0.9659 - val_accuracy: 0.8070\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.2359 - accuracy: 0.9489 - val_loss: 0.9162 - val_accuracy: 0.8150\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.2037 - accuracy: 0.9505 - val_loss: 0.9455 - val_accuracy: 0.8100\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1781 - accuracy: 0.9542 - val_loss: 0.9455 - val_accuracy: 0.8210\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1500 - accuracy: 0.9568 - val_loss: 1.0259 - val_accuracy: 0.8020\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1525 - accuracy: 0.9571 - val_loss: 1.0090 - val_accuracy: 0.7990\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1358 - accuracy: 0.9566 - val_loss: 1.0177 - val_accuracy: 0.8040\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.1202 - accuracy: 0.9620 - val_loss: 1.0559 - val_accuracy: 0.7960\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.1101 - accuracy: 0.9628 - val_loss: 1.0545 - val_accuracy: 0.8040\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.1066 - accuracy: 0.9630 - val_loss: 1.0828 - val_accuracy: 0.7970\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1125 - accuracy: 0.9604 - val_loss: 1.1353 - val_accuracy: 0.7960\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.1091 - accuracy: 0.9611 - val_loss: 1.1505 - val_accuracy: 0.7830\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display its loss and accuracy curves:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们来绘制损失曲线和精度曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAArcklEQVR4nO3deZwU1bn/8c/DIjhsKuAGMgMGQRAYYFgEJRjNjeBOTBSJinhFjIlrFBNulF9yyTUJSby4xGCMxkhEExOuC0YDQgB3QERQjKigCCGIAoMssjy/P04N09N0z8JMTfdMf9+vV726urZ+uqbnPFXnVJ0yd0dERHJXg0wHICIimaVEICKS45QIRERynBKBiEiOUyIQEclxSgQiIjlOiUBqlJk9Y2aX1vSymWRmq8zstBi262b2pWj8XjP7YWWWPYDPGWVmzx1onOVsd6iZranp7Urta5TpACTzzGxrwts8YCewJ3p/pbtPq+y23H1YHMvWd+4+ria2Y2YFwAdAY3ffHW17GlDpv6HkHiUCwd2bl4yb2SrgP919VvJyZtaopHARkfpDVUOSVsmpv5mNN7N/AQ+Y2aFm9pSZbTCzz6Lx9gnrzDWz/4zGR5vZAjObHC37gZkNO8BlO5rZPDMrNrNZZna3mT2cJu7KxPhjM3sh2t5zZtYmYf7FZrbazDaa2YRy9s9AM/uXmTVMmHaemS2Nxvub2UtmtsnM1pnZXWZ2UJptPWhm/53w/qZonbVmNiZp2TPM7HUz22JmH5nZxITZ86LXTWa21cxOLNm3CesPMrPXzGxz9DqosvumPGZ2fLT+JjNbbmZnJ8wbbmZvRdv82My+F01vE/19NpnZp2Y238xULtUy7XCpyJHAYUA+MJbwm3kget8B2A7cVc76A4B3gDbAz4D7zcwOYNk/Aq8CrYGJwMXlfGZlYrwIuAw4HDgIKCmYugG/jrZ/dPR57UnB3V8GPge+krTdP0bje4Dro+9zInAq8O1y4iaK4fQonq8CnYHk9onPgUuAQ4AzgKvM7Nxo3pDo9RB3b+7uLyVt+zDgaWBK9N1+CTxtZq2TvsN++6aCmBsDTwLPRet9F5hmZl2iRe4nVDO2AE4Ano+m3wisAdoCRwA/ANTvTS1TIpCK7AVuc/ed7r7d3Te6++Puvs3di4FJwJfLWX+1u9/n7nuA3wNHEf7hK72smXUA+gG3uvsX7r4AeCLdB1Yyxgfc/Z/uvh14DCiMpp8PPOXu89x9J/DDaB+k8wgwEsDMWgDDo2m4+yJ3f9ndd7v7KuA3KeJI5ZtRfMvc/XNC4kv8fnPd/U133+vuS6PPq8x2ISSOd939D1FcjwArgLMSlkm3b8ozEGgO3B79jZ4HniLaN8AuoJuZtXT3z9x9ccL0o4B8d9/l7vNdHaDVOiUCqcgGd99R8sbM8szsN1HVyRZCVcQhidUjSf5VMuLu26LR5lVc9mjg04RpAB+lC7iSMf4rYXxbQkxHJ247Kog3pvsswtH/CDNrAowAFrv76iiO46Jqj39FcfyEcHZQkTIxAKuTvt8AM5sTVX1tBsZVcrsl216dNG010C7hfbp9U2HM7p6YNBO3+3VCklxtZv8wsxOj6T8HVgLPmdn7ZnZL5b6G1CQlAqlI8tHZjUAXYIC7t6S0KiJddU9NWAccZmZ5CdOOKWf56sS4LnHb0We2Trewu79FKPCGUbZaCEIV0wqgcxTHDw4kBkL1VqI/Es6IjnH3VsC9Cdut6Gh6LaHKLFEH4ONKxFXRdo9Jqt/ft113f83dzyFUG80gnGng7sXufqO7dyKcldxgZqdWMxapIiUCqaoWhDr3TVF9821xf2B0hL0QmGhmB0VHk2eVs0p1YvwzcKaZnRQ17P6Iiv9P/ghcQ0g4f0qKYwuw1cy6AldVMobHgNFm1i1KRMnxtyCcIe0ws/6EBFRiA6Eqq1Oabc8EjjOzi8yskZldAHQjVONUxyuEtoubzayxmQ0l/I2mR3+zUWbWyt13EfbJHgAzO9PMvhS1BZVM35PyEyQ2SgRSVXcABwOfAC8Df6ulzx1FaHDdCPw38CjhfodU7uAAY3T35cDVhMJ9HfAZoTGzPI8AQ4Hn3f2ThOnfIxTSxcB9UcyVieGZ6Ds8T6g2eT5pkW8DPzKzYuBWoqPraN1thDaRF6IrcQYmbXsjcCbhrGkjcDNwZlLcVebuXwBnE86MPgHuAS5x9xXRIhcDq6IqsnHAt6LpnYFZwFbgJeAed59bnVik6kztMlIXmdmjwAp3j/2MRKS+0xmB1Alm1s/MjjWzBtHllecQ6ppFpJp0Z7HUFUcCfyE03K4BrnL31zMbkkj9oKohEZEcp6ohEZEcV+eqhtq0aeMFBQWZDkNEpE5ZtGjRJ+7eNtW8OpcICgoKWLhwYabDEBGpU8ws+Y7yfVQ1JCKS45QIRERynBKBiEiOq3NtBCJS+3bt2sWaNWvYsWNHxQtLRjVt2pT27dvTuHHjSq+jRCAiFVqzZg0tWrSgoKCA9M8VkkxzdzZu3MiaNWvo2LFjpdfLiaqhadOgoAAaNAiv0/QYb5Eq2bFjB61bt1YSyHJmRuvWrat85lbvzwimTYOxY2Fb9EiT1avDe4BRozIXl0hdoyRQNxzI36nenxFMmFCaBEps2xami4hIDiSCDz+s2nQRyT4bN26ksLCQwsJCjjzySNq1a7fv/RdffFHuugsXLuSaa66p8DMGDRpUI7HOnTuXM888s0a2VVvqfSLokPyQvwqmi0j11XS7XOvWrVmyZAlLlixh3LhxXH/99fveH3TQQezevTvtukVFRUyZMqXCz3jxxRerF2QdVu8TwaRJkJdXdlpeXpguIjWvpF1u9WpwL22Xq+mLNEaPHs0NN9zAKaecwvjx43n11VcZNGgQvXv3ZtCgQbzzzjtA2SP0iRMnMmbMGIYOHUqnTp3KJIjmzZvvW37o0KGcf/75dO3alVGjRlHSS/PMmTPp2rUrJ510Etdcc02FR/6ffvop5557Lj179mTgwIEsXboUgH/84x/7zmh69+5NcXEx69atY8iQIRQWFnLCCScwf/78mt1h5aj3jcUlDcITJoTqoA4dQhJQQ7FIPMprl6vp/7t//vOfzJo1i4YNG7JlyxbmzZtHo0aNmDVrFj/4wQ94/PHH91tnxYoVzJkzh+LiYrp06cJVV1213zX3r7/+OsuXL+foo49m8ODBvPDCCxQVFXHllVcyb948OnbsyMiRIyuM77bbbqN3797MmDGD559/nksuuYQlS5YwefJk7r77bgYPHszWrVtp2rQpU6dO5Wtf+xoTJkxgz549bEveiTGq94kAwo9PBb9I7ajNdrlvfOMbNGzYEIDNmzdz6aWX8u6772Jm7Nq1K+U6Z5xxBk2aNKFJkyYcfvjhrF+/nvbt25dZpn///vumFRYWsmrVKpo3b06nTp32XZ8/cuRIpk6dWm58CxYs2JeMvvKVr7Bx40Y2b97M4MGDueGGGxg1ahQjRoygffv29OvXjzFjxrBr1y7OPfdcCgsLq7NrqqTeVw2JSO2qzXa5Zs2a7Rv/4Q9/yCmnnMKyZct48skn015L36RJk33jDRs2TNm+kGqZA3mIV6p1zIxbbrmF3/72t2zfvp2BAweyYsUKhgwZwrx582jXrh0XX3wxDz30UJU/70ApEYhIjcpUu9zmzZtp164dAA8++GCNb79r1668//77rFq1CoBHH320wnWGDBnCtKhxZO7cubRp04aWLVvy3nvv0aNHD8aPH09RURErVqxg9erVHH744VxxxRVcfvnlLF68uMa/QzpKBCJSo0aNgqlTIT8fzMLr1KnxV8/efPPNfP/732fw4MHs2bOnxrd/8MEHc88993D66adz0kknccQRR9CqVaty15k4cSILFy6kZ8+e3HLLLfz+978H4I477uCEE06gV69eHHzwwQwbNoy5c+fuazx+/PHHufbaa2v8O6RT555ZXFRU5HowjUjtevvttzn++OMzHUbGbd26lebNm+PuXH311XTu3Jnrr78+02HtJ9Xfy8wWuXtRquV1RiAiUkn33XcfhYWFdO/enc2bN3PllVdmOqQakRNXDYmI1ITrr78+K88Aqiu2MwIzO8bM5pjZ22a23Mz2q/Ays6FmttnMlkTDrXHFIyIiqcV5RrAbuNHdF5tZC2CRmf3d3d9KWm6+u9etjjlEROqR2M4I3H2duy+OxouBt4F2cX2eiIgcmFppLDazAqA38EqK2Sea2Rtm9oyZda+NeEREpFTsicDMmgOPA9e5+5ak2YuBfHfvBdwJzEizjbFmttDMFm7YsCHWeEUk+wwdOpRnn322zLQ77riDb3/72+WuU3Kp+fDhw9m0adN+y0ycOJHJkyeX+9kzZszgrbdKa7RvvfVWZs2aVYXoU8um7qpjTQRm1piQBKa5+1+S57v7FnffGo3PBBqbWZsUy0119yJ3L2rbtm2cIYtIFho5ciTTp08vM2369OmV6vgNQq+hhxxyyAF9dnIi+NGPfsRpp512QNvKVnFeNWTA/cDb7v7LNMscGS2HmfWP4tkYV0wiUjedf/75PPXUU+zcuROAVatWsXbtWk466SSuuuoqioqK6N69O7fddlvK9QsKCvjkk08AmDRpEl26dOG0007b11U1hHsE+vXrR69evfj617/Otm3bePHFF3niiSe46aabKCws5L333mP06NH8+c9/BmD27Nn07t2bHj16MGbMmH3xFRQUcNttt9GnTx969OjBihUryv1+me6uOs6rhgYDFwNvmtmSaNoPgA4A7n4vcD5wlZntBrYDF3pdu9VZJMdcdx0sWVKz2ywshDvuSD+/devW9O/fn7/97W+cc845TJ8+nQsuuAAzY9KkSRx22GHs2bOHU089laVLl9KzZ8+U21m0aBHTp0/n9ddfZ/fu3fTp04e+ffsCMGLECK644goA/uu//ov777+f7373u5x99tmceeaZnH/++WW2tWPHDkaPHs3s2bM57rjjuOSSS/j1r3/NddddB0CbNm1YvHgx99xzD5MnT+a3v/1t2u+X6e6q47xqaIG7m7v3dPfCaJjp7vdGSQB3v8vdu7t7L3cf6O65+4ggESlXYvVQYrXQY489Rp8+fejduzfLly8vU42TbP78+Zx33nnk5eXRsmVLzj777H3zli1bxsknn0yPHj2YNm0ay5cvLzeed955h44dO3LccccBcOmllzJv3rx980eMGAFA375993VUl86CBQu4+OKLgdTdVU+ZMoVNmzbRqFEj+vXrxwMPPMDEiRN58803adGiRbnbrgzdWSwiVVLekXuczj33XG644QYWL17M9u3b6dOnDx988AGTJ0/mtdde49BDD2X06NFpu58uEdVG72f06NHMmDGDXr168eCDDzJ37txyt1NR5UVJV9bpurquaFsl3VWfccYZzJw5k4EDBzJr1qx93VU//fTTXHzxxdx0001ccskl5W6/IuprSETqhObNmzN06FDGjBmz72xgy5YtNGvWjFatWrF+/XqeeeaZcrcxZMgQ/vrXv7J9+3aKi4t58skn980rLi7mqKOOYteuXfu6jgZo0aIFxcXF+22ra9eurFq1ipUrVwLwhz/8gS9/+csH9N0y3V21zghEpM4YOXIkI0aM2FdF1KtXL3r37k337t3p1KkTgwcPLnf9Pn36cMEFF1BYWEh+fj4nn3zyvnk//vGPGTBgAPn5+fTo0WNf4X/hhRdyxRVXMGXKlH2NxABNmzblgQce4Bvf+Aa7d++mX79+jBs37oC+18SJE7nsssvo2bMneXl5ZbqrnjNnDg0bNqRbt24MGzaM6dOn8/Of/5zGjRvTvHnzGnmAjbqhFpEKqRvqukXdUIuISJUoEYiI5DglAhGplLpWjZyrDuTvpEQgIhVq2rQpGzduVDLIcu7Oxo0badq0aZXW01VDIlKh9u3bs2bNGtTpY/Zr2rQp7du3r9I6SgQiUqHGjRvTsWPHTIchMVHVkIhIjlMiEBHJcUoEIiI5TolARCTHKRGIiOQ4JQIRkRynRCAikuOUCEREcpwSgYhIjlMiEBHJcUoEIiI5TolARCTHKRGIiOQ4JQIRkRynRCAikuOUCEREcpwSgYhIjlMiEBHJcUoEIiI5TolARCTHxZYIzOwYM5tjZm+b2XIzuzbFMmZmU8xspZktNbM+ccUjIiKpNYpx27uBG919sZm1ABaZ2d/d/a2EZYYBnaNhAPDr6FVERGpJbGcE7r7O3RdH48XA20C7pMXOAR7y4GXgEDM7Kq6YRERkf7XSRmBmBUBv4JWkWe2AjxLer2H/ZIGZjTWzhWa2cMOGDbHFKSKSi2JPBGbWHHgcuM7dtyTPTrGK7zfBfaq7F7l7Udu2beMIU0QkZ8WaCMysMSEJTHP3v6RYZA1wTML79sDaOGMSEZGy4rxqyID7gbfd/ZdpFnsCuCS6emggsNnd18UVk4iI7C/Oq4YGAxcDb5rZkmjaD4AOAO5+LzATGA6sBLYBl8UYj4iIpBBbInD3BaRuA0hcxoGr44pBREQqpjuLRURynBKBiEiOUyIQEclxSgQiIjlOiUBEJMcpEYiI5DglAhGRHKdEICKS45QIRERynBKBiEiOy5lEsHw5jB0LO3dmOhIRkeySM4ng44/hvvvgj3/MdCQiItklZxLBV78KvXvDz34Ge/dmOhoRkeyRM4nADG6+GVasgCefzHQ0IiLZI2cSAcD550PHjvDTn4Lv90BMEZHclFOJoFEjuPFGeOklWLAg09GIiGSHnEoEAJddBm3ahLYCERHJwUSQlwfXXANPPQXLlmU6GhGRzMu5RADw7W+HhPDzn2c6EhGRzMvJRNC6NVxxRbin4MMPMx2NiEhm5WQiALjhhvD6q19lNg4RkUzL2UTQoQOMHBnuNv7000xHIyKSOTmbCABuugk+/xzuuSfTkYiIZE5OJ4IePWD4cJgyBbZvz3Q0IiKZkdOJAGD8eNiwAR54INORiIhkRs4ngpNPhoEDYfJk2L0709GIiNS+nE8EZuGs4IMP4PHHMx2NiEjty/lEAHD22dClizqjE5HcpEQANGgQriB6/XWYNSvT0YiI1K7YEoGZ/c7M/m1mKXv0MbOhZrbZzJZEw61xxVIZ3/oWHH10OCsQEcklcZ4RPAicXsEy8929MBp+FGMsFWrSBK67DmbPhkWLMhmJiEjtii0RuPs8oE7ds3vlldCypbqoFpHcUqlEYGbNzKxBNH6cmZ1tZo1r4PNPNLM3zOwZM+tezuePNbOFZrZww4YNNfCxqbVsCVddBX/+M7z3XmwfIyKSVSp7RjAPaGpm7YDZwGWEqp/qWAzku3sv4E5gRroF3X2quxe5e1Hbtm2r+bHlu/ba8CSzyZNj/RgRkaxR2URg7r4NGAHc6e7nAd2q88HuvsXdt0bjM4HGZtamOtusCUcdBZdeGu40Xr8+09GIiMSv0onAzE4ERgFPR9MaVeeDzexIM7NovH8Uy8bqbLOmfO978MUXcOedmY5ERCR+lU0E1wHfB/7q7svNrBMwp7wVzOwR4CWgi5mtMbPLzWycmY2LFjkfWGZmbwBTgAvds+N2ruOOgxEj4O67obgYpk2DgoJwv0FBQXgvIlJfWFXL3qjRuLm7b4knpPIVFRX5woULY/+cV1+FAQPgootgxgzYtq10Xl4eTJ0Ko0bFHoaISI0ws0XuXpRqXmWvGvqjmbU0s2bAW8A7ZnZTTQaZbfr3h6FD4dFHyyYBCO8nTMhIWCIiNa6yVUPdojOAc4GZQAfg4riCyhbjx8OePann6VnHIlJfVDYRNI7uGzgX+D933wVkRX1+nL72NWic5m6JDh1qNxYRkbhUNhH8BlgFNAPmmVk+kJE2gtpkBv/5n/tPz8uDSZNqPx4RkThUubF434pmjdy91h/lUluNxSV27w73FhQXh0tKO3QISUANxSJSl9REY3ErM/tlSTcPZvYLwtlBvdeoEdx6K+zcCfPnw6pVSgIiUr9Utmrod0Ax8M1o2ALkzFN+x4yB1q3VGZ2I1E+VTQTHuvtt7v5+NPw/oFOcgWWTZs3gu9+FJ56A6dMzHY2ISM2qbCLYbmYnlbwxs8HA9nhCyk433BAedD9yJPziF3qkpYjUH5XtL2gc8JCZtYrefwZcGk9I2alFC3juObjkktAX0UcfhYTQsGGmIxMRqZ5KJQJ3fwPoZWYto/dbzOw6YGmMsWWdpk1D1VC7dnDHHbBmDTz8cJguIlJXVekJZVHX0SX3D9wQQzxZr0ED+NWv4Je/hMcfh69+FT6tU89hExEpqzqPqrQai6IOuv760A/Rq6/CSSfB6tWZjkhE5MBUJxHkfHPpN78Z2g3WrYOBA2HJkkxHJCJSdeUmAjMrNrMtKYZi4OhaijGrffnLsGBB6JPo5JPh73/PdEQiIlVTbiJw9xbu3jLF0MLdq/WEsvqke3d46SXo1AmGD4eHHsp0RCIilVedqiFJ0K4dzJsXzhAuvRR+8hPdayAidYMSQQ1q1Qpmzgx9EU2YAFdfnf55BiIi2ULVOzXsoINC1dAxx8Dtt8PHH8Mjj4Suq0VEspHOCGLQoAH8z//AXXfBk0/CqafCJ59kOioRkdR0RhCjq68ObQcjR8KgQfDMM3DssZmOSkSy2a5d4SbVTz+FjRtLXzduhKKi8Cz1mqZEELNzz4XZs+Gss+DEE0PXFCNHhqefiUhu2LYt3Hz60UepC/jEaVvKefbjjTcqEdRZgwbBiy/CRReFhuQ77wwJYcCATEcmInHYvh1efhnmzAnDK6+EI/0SZnDooXDYYeFZJ4cfDscfH8ZLpiWOl7y2aBFPvEoEtaRLF3jttdCQ/P3vhzuRv/Wt0JbQvn2moxOR6ti5MxT8c+eGgv/ll8O0Bg2gTx+47rpwJN+5cyjUDzkku3ouPuBnFmdKbT+zOA7FxfDTn8LkyeGHMn483HSTriwSqSu++CJU9cyZEwr/F1+EHTvCkX7v3nDKKaHgP/nkcFl5NijvmcVKBBm0alVIAo89Fs4Kbr89tB800LVcIlnDPfQn9u678MILofB/4YVQ/WMGvXqVFvxDhoSj/WykRJDlFiwIp46LFoV2gzvuCFVHIlI7du4MB2bvvQfvvx9eS8bffz8U+iV69gyF/imnhIL/sMMyFXXVlJcI1EaQBU46KZxm/uEPof3gxBNDw/Ltt4cb00Ry3a5doWAuqX5p0KD0tTLjJVfprV1btpAvGV+zpmyXMHl54VLvL30Jvva1MN6pU7h8s02bzOyDOOmMoBZMmxa6nPjwQ+jQASZNClcPpbJ1a2n7gVloO7j5ZmjWrHZjFsmUTz+FN94oOyxfHurla9KRR4bC/dhjSwv6ktcjjqh/l3iraiiDpk2DsWPDdcQl8vJg6tT0yQDCg25uuaX00Zi33x7OEtR+IPXFnj2h3j2xwF+6NBydlzjiiFAH36sX9OgRLp/cuzccvSe+phtPnnbUUaGg79Qp9w6uMpIIzOx3wJnAv939hBTzDfhfYDiwDRjt7osr2m5dSwQFBamfXpafH+okK/LCC+FpaK+9Fk5LR40Kp6pdu9a/IxaJx+rV8MAD8Kc/hQOJNm2gbduyQ/K01q3DMzYOxN69oU7988/DsG1beC0uhrffLi30ly0rrXtv1Cj8pksK/ZLhiCNqbj/kukwlgiHAVuChNIlgOPBdQiIYAPyvu1d4i1VdSwQNGqTujtos/MNUxt694cziJz+BFSvCtPbt4T/+IySFU08N/7giJXbsgBkz4P77w53tEBo3W7aEDRtKh88+S7+NQw7ZPzns3l1asCcW8onjiQ2rqbRuXbaw79kTunWDJk1q6ttLKhmrGjKzAuCpNIngN8Bcd38kev8OMNTd15W3zbqWCKp7RpBs1arweMznnoNZs2Dz5pBU+vUrTQwDBhz40ZzUbUuWhMJ/2rRQyOfnw2WXwejRYTzZ7t2ha4OSxPDJJ2UTReL7jRvD7yovL1SrNGtW9fHOneHoo3U2mwnZmgieAm539wXR+9nAeHffr5Q3s7HAWIAOHTr0XV2HnhR/oG0ElbF7d6gyeu45ePbZcBv73r3hqO8rXylNDJ06Ve9zJLt99lno6vz++2Hx4nBkfd55cPnl4XegdiWB7L18NNUxQcqs5O5TgakQzgjiDKqmlRT2lb1qqCoaNQqXmp54Itx2G2zaFKoBShLDjBlhuWOPDQnhq1+Fvn1DtVKuHZFt3w6vvx4uBzz88ExHU31794Y7Wu+/H/7yl1AVVFgY+rG66KK6c227ZAdVDdVT7uGKjGefDYlhzpxQfwuh7rdnz7LDCSfUv6so1q6Fp58Oz4SYNau07rpLl3Drf8lQUFB3EuNHH8GDD4bG3w8+CN0XjBoVjv779Ml0dJLNsrVq6AzgO5Q2Fk9x9/4VbVOJ4MDs3AkLF5Zeord0Kbz5ZrhvAUJBeOyxZZNDr16hkKxu1cKuXSEJffFFuDolrqoK91A18tRTofBftChM79AhdAN+yimwciXMnx+uxtq0Kcxv165sYujevWa+8+rV4fNKhlWryvZAWVVbtoS43UOVz+WXhyqggw+uXqySGzJ11dAjwFCgDbAeuA1oDODu90aXj94FnE64fPSyVO0DyZQIas7evaFwKkkMJcnh3XdLr3Rq3jxcv92zZ7i8b+/ekDxKrhT5/POK3yfeCNSsWehut1u30uH446FjxwPrjXHbtlAd9tRTYVi7NiS1gQND4X/mmeFsJ/mIf+/ecPni/Pmlw9q1Yd6hh8LgwaWJoW/f8AjSZDt2hKPylSvD3anJhX7i86qbNQvfsWnTqn/HEg0bhnaf0aPV7iNVpxvKpEo+/xzeeqtsgnjjjbKXGjZpUnolSPPmpeOJQ/L0Ro1CgfnWW2EoKXhLtte1a9kE0a1bOEtJvgLq449LC/5Zs0KB3Lx5aAc56ywYNqzq7QDuoVBPTAz//GeYd/DB4UqsAQPCPigp7D/6qOylwa1ahTaIVEN9vFNV6hYlAqk293Drf8nlg41q4DKDzZvDDUYliaFkSLworHFjOO64cNbQrl0ooBdHtx0WFISC/6yzQudfNX0d+vr1oUPAksSwZElohE1X2B92mAp7yV5KBFKnbN0K77yTOkH061da+HfrVrsF75492fUwEZGqyNbLR0VSat481Mv37Vt2untmj7iVBKS+0q0mUmeo2kUkHkoEIiI5TolARCTHKRHUAdOmld7YVVAQ3ouI1BQ1Fme55E7rVq8O76Fm+isSEdEZQZabMKFsz6UQ3k+YkJl4RKT+USLIch9+WLXpIiJVpUSQ5Tp0qNp0EZGqUiLIcpMmhS4dEuXlhekiIjVBiSDLjRoVnmaWnx9uqMrPr5mnm4mIlNBVQ3XAqFEq+EUkPjojEBHJcUoEIiI5TolARCTHKRGIiOQ4JYIcoL6KRKQ8umqonlNfRSJSEZ0R1HPqq0hEKqJEUM+pryIRqYgSQT2nvopEpCJKBPWc+ioSkYooEdRz6qtIRCqiq4ZygPoqEpHy6IxARCTHKRGIiOQ4JQKpFN2dLFJ/xZoIzOx0M3vHzFaa2S0p5g81s81mtiQabo0zHjkwJXcnr14N7qV3JysZiNQPsSUCM2sI3A0MA7oBI82sW4pF57t7YTT8KK545MDp7mSR+i3OM4L+wEp3f9/dvwCmA+fE+HkSE92dLFK/xZkI2gEfJbxfE01LdqKZvWFmz5hZ91QbMrOxZrbQzBZu2LAhjlilHLo7WaR+izMRWIppnvR+MZDv7r2AO4EZqTbk7lPdvcjdi9q2bVuzUUqFdHeySP0WZyJYAxyT8L49sDZxAXff4u5bo/GZQGMzaxNjTHIAdHeySP0W553FrwGdzawj8DFwIXBR4gJmdiSw3t3dzPoTEtPGGGOSA6S7k0Xqr9jOCNx9N/Ad4FngbeAxd19uZuPMbFy02PnAMjN7A5gCXOjuydVHUg/oPgSR7GV1rdwtKiryhQsXZjoMqYLkp6RBaGNQ9ZJI7TGzRe5elGqe7iyW2Ok+BJHspkQgsdN9CCLZTYlAYqf7EESymxKBxK4m7kNQY7NIfJQIJHbVvQ9Bnd6JxEtXDUnWKygIhX+y/HxYtaq2oxGpm3TVkNRpamwWiZcSgWQ9NTaLxEuJQLKeGptF4qVEIFlPjc0i8VJjsdR7amwWUWOx5LiaaGxW1ZLUZ0oEUu9Vt7FZVUtS3ykRSL1X3cZmdZon9Z0SgdR71W1sVtWS1HdKBJITRo0KDcN794bXqjwHIRuqlpRIJE5KBCIVyHTVktooJG5KBCIVyHTVUk20UeiMQsqjRCBSCZmsWqpuIlHVlFREiUAkZtWtWqpuIsmGqqnqJhIlopi5e50a+vbt6yJ1zcMPu+fnu5uF14cfrtq6eXnuoRgOQ15e5bdhVnbdksGscuvn56dePz+/duKv7vol2zjQ/V8T62cDYKGnKVczXrBXdVAikFxUnYKougV5phOJElHNJCIlApEcVt2CLNOJRImo+onIvfxEoDYCkXquulc9ZbqNI9ON7Zm+6qs27mxXIhDJAdW56inTiUSJqHrrV4YSgYhUKJOJRImoeutXSro6o2wd1EYgIlWVycbautBGoAfTiIjEbNq0UKf/4YfhSH7SpKqdVVV3fSj/wTRKBCIiOUBPKBMRkbRiTQRmdrqZvWNmK83slhTzzcymRPOXmlmfOOMREZH9xZYIzKwhcDcwDOgGjDSzbkmLDQM6R8NY4NdxxSMiIqnFeUbQH1jp7u+7+xfAdOCcpGXOAR6KGrVfBg4xs6NijElERJLEmQjaAR8lvF8TTavqMpjZWDNbaGYLN2zYUOOBiojkskYxbttSTEu+RKkyy+DuU4GpAGa2wcxWVz+8WLQBPsl0EOXI9vgg+2NUfNWj+KqnOvHlp5sRZyJYAxyT8L49sPYAlinD3dvWSHQxMLOF6S7PygbZHh9kf4yKr3oUX/XEFV+cVUOvAZ3NrKOZHQRcCDyRtMwTwCXR1UMDgc3uvi7GmEREJElsZwTuvtvMvgM8CzQEfufuy81sXDT/XmAmMBxYCWwDLosrHhERSS3OqiHcfSahsE+cdm/CuANXxxlDLZua6QAqkO3xQfbHqPiqR/FVTyzx1bkuJkREpGapiwkRkRynRCAikuOUCKrIzI4xszlm9raZLTeza1MsM9TMNpvZkmi4tZZjXGVmb0afvV9XrZns48nMuiTslyVmtsXMrktaptb3n5n9zsz+bWbLEqYdZmZ/N7N3o9dD06xbbp9aMcb3czNbEf0N/2pmh6RZt9zfQ4zxTTSzjxP+jsPTrJup/fdoQmyrzGxJmnVj3X/pypRa/f2le1CBhtQDcBTQJxpvAfwT6Ja0zFDgqQzGuApoU8784cAzhBv6BgKvZCjOhsC/gPxM7z9gCNAHWJYw7WfALdH4LcBP03yH94BOwEHAG8m/hxjj+w+gUTT+01TxVeb3EGN8E4HvVeI3kJH9lzT/F8Ctmdh/6cqU2vz96Yygitx9nbsvjsaLgbdJ0S1GlsuWPp5OBd5z94zfKe7u84BPkyafA/w+Gv89cG6KVSvTp1Ys8bn7c+6+O3r7MuGGzIxIs/8qI2P7r4SZGfBN4JGa/tzKKKdMqbXfnxJBNZhZAdAbeCXF7BPN7A0ze8bMutduZDjwnJktMrOxKeZXqo+nWnAh6f/5Mrn/Shzh0Q2O0evhKZbJln05hnCWl0pFv4c4fSequvpdmqqNbNh/JwPr3f3dNPNrbf8llSm19vtTIjhAZtYceBy4zt23JM1eTKju6AXcCcyo5fAGu3sfQjffV5vZkKT5lerjKU4W7jY/G/hTitmZ3n9VkQ37cgKwG5iWZpGKfg9x+TVwLFAIrCNUvyTL+P4DRlL+2UCt7L8KypS0q6WYVuX9p0RwAMysMeEPNs3d/5I83923uPvWaHwm0NjM2tRWfO6+Nnr9N/BXwuljoir38RSDYcBid1+fPCPT+y/B+pIqs+j13ymWyei+NLNLgTOBUR5VGierxO8hFu6+3t33uPte4L40n5vp/dcIGAE8mm6Z2th/acqUWvv9KRFUUVSfeD/wtrv/Ms0yR0bLYWb9Cft5Yy3F18zMWpSMExoUlyUtlg19PKU9Csvk/kvyBHBpNH4p8H8plqlMn1qxMLPTgfHA2e6+Lc0ylfk9xBVfYrvTeWk+N2P7L3IasMLd16SaWRv7r5wypfZ+f3G1hNfXATiJcOq1FFgSDcOBccC4aJnvAMsJLfgvA4NqMb5O0ee+EcUwIZqeGJ8Rnh73HvAmUFTL+zCPULC3SpiW0f1HSErrgF2Eo6zLgdbAbODd6PWwaNmjgZkJ6w4nXOnxXsn+rqX4VhLqh0t+h/cmx5fu91BL8f0h+n0tJRROR2XT/oumP1jyu0tYtlb3XzllSq39/tTFhIhIjlPVkIhIjlMiEBHJcUoEIiI5TolARCTHKRGIiOQ4JQKRiJntsbI9o9ZYT5hmVpDY86VINon1UZUidcx2dy/MdBAitU1nBCIViPqj/6mZvRoNX4qm55vZ7KhTtdlm1iGafoSF5wO8EQ2Dok01NLP7oj7nnzOzg6PlrzGzt6LtTM/Q15QcpkQgUurgpKqhCxLmbXH3/sBdwB3RtLsI3Xn3JHT4NiWaPgX4h4dO8/oQ7kgF6Azc7e7dgU3A16PptwC9o+2Mi+eriaSnO4tFIma21d2bp5i+CviKu78fdQ72L3dvbWafELpN2BVNX+fubcxsA9De3XcmbKMA+Lu7d47ejwcau/t/m9nfgK2EXlZneNThnkht0RmBSOV4mvF0y6SyM2F8D6VtdGcQ+n7qCyyKesQUqTVKBCKVc0HC60vR+IuE3h4BRgELovHZwFUAZtbQzFqm26iZNQCOcfc5wM3AIcB+ZyUicdKRh0ipg63sA8z/5u4ll5A2MbNXCAdPI6Np1wC/M7ObgA3AZdH0a4GpZnY54cj/KkLPl6k0BB42s1aEXmF/5e6bauj7iFSK2ghEKhC1ERS5+yeZjkUkDqoaEhHJcTojEBHJcTojEBHJcUoEIiI5TolARCTHKRGIiOQ4JQIRkRz3/wHH3iDdTjzpRAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # clear figure\n",
    "\n",
    "acc=history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the network starts overfitting after 8 epochs. Let's train a new network from scratch for 8 epochs, then let's evaluate it on \n",
    "the test set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络在训练 9 轮后开始过拟合。我们从头开始训练一个新网络，共 10 个轮次，然后在测试\n",
    "集上评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 1s 27ms/step - loss: 3.1485 - accuracy: 0.3866 - val_loss: 1.7549 - val_accuracy: 0.6480\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 1.4787 - accuracy: 0.7084 - val_loss: 1.2705 - val_accuracy: 0.7350\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.0737 - accuracy: 0.7779 - val_loss: 1.1174 - val_accuracy: 0.7500\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.8294 - accuracy: 0.8194 - val_loss: 1.0326 - val_accuracy: 0.7750\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6755 - accuracy: 0.8545 - val_loss: 0.9467 - val_accuracy: 0.7970\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.5344 - accuracy: 0.8865 - val_loss: 0.9119 - val_accuracy: 0.8150\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.4292 - accuracy: 0.9131 - val_loss: 0.8953 - val_accuracy: 0.8100\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.3405 - accuracy: 0.9304 - val_loss: 0.9206 - val_accuracy: 0.8010\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.2972 - accuracy: 0.9363 - val_loss: 0.8807 - val_accuracy: 0.8100\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.2284 - accuracy: 0.9482 - val_loss: 0.9135 - val_accuracy: 0.8120\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 1.0005 - accuracy: 0.7858\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=10,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, one_hot_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0005097389221191, 0.7858415246009827]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "#做一个混淆矩阵\n",
    "prediction=model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 10,  1, ...,  3,  4,  1], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction #预估结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 10,  1, ...,  3,  3, 24], dtype=int64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels #标准答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predict</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>767</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>411</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "predict  0   1   2    3    4   6   7   8   9   10  ...  33  34  35  36  37  \\\n",
       "labels                                             ...                       \n",
       "0         7   2   0    1    1   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "1         0  87   0    4    6   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "2         0   3  13    1    0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "3         0   5   1  767   26   0   0   2   1   0  ...   0   0   0   0   0   \n",
       "4         1   5   0   28  411   1   0   0   0   0  ...   0   0   0   1   0   \n",
       "5         0   5   0    0    0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "6         0   1   0    0    1  12   0   0   0   0  ...   0   0   0   0   0   \n",
       "7         0   1   0    1    0   0   1   0   0   0  ...   0   0   0   0   0   \n",
       "8         0   0   0    4    5   0   0  25   0   0  ...   0   0   0   0   0   \n",
       "9         0   1   0    0    2   0   0   0  18   0  ...   0   0   0   0   0   \n",
       "10        0   4   0    0    0   0   0   0   0  24  ...   0   0   0   0   0   \n",
       "11        0   2   0    3    5   0   0   0   1   0  ...   0   1   0   1   0   \n",
       "12        0   0   0    1    0   0   0   1   0   0  ...   0   0   0   0   0   \n",
       "13        0   1   0    1    2   0   0   0   0   1  ...   0   0   0   0   0   \n",
       "14        0   1   0    0    0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "15        0   2   0    4    0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "16        0   0   0    8    6   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "17        0   0   0    1    2   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "18        0   0   0    1    0   0   0   1   0   0  ...   0   0   0   0   0   \n",
       "19        0   2   0    8    2   0   0   3   0   0  ...   0   0   0   0   0   \n",
       "20        0   1   0    7    3   0   0   2   0   0  ...   0   0   0   0   0   \n",
       "21        0   1   0    0    0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "22        0   1   1    2    2   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "23        0   1   0    3    0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "24        0   6   2    2    1   0   0   0   1   1  ...   0   0   0   0   0   \n",
       "25        0   0   0    2    2   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "26        0   0   0    0    0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "27        0   0   0    0    0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "28        0   4   0    0    2   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "29        0   0   0    0    1   0   0   1   0   0  ...   0   0   0   0   0   \n",
       "30        0   0   0    1    0   0   0   0   0   0  ...   0   0   0   1   0   \n",
       "31        0   0   0    4    1   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "32        0   0   0    1    0   0   0   0   0   1  ...   0   0   0   0   0   \n",
       "33        0   0   0    0    0   0   0   0   0   0  ...   3   0   0   0   0   \n",
       "34        0   0   0    1    1   0   0   0   0   0  ...   0   4   0   0   0   \n",
       "35        0   0   0    0    0   0   0   1   0   0  ...   0   0   1   0   0   \n",
       "36        0   0   0    1    0   0   0   0   0   0  ...   0   0   0   3   0   \n",
       "37        0   0   0    1    1   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "38        0   1   0    0    0   0   0   2   0   0  ...   0   0   0   0   0   \n",
       "39        0   0   0    0    1   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "40        0   0   0    0    0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "41        0   0   0    0    6   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "42        0   0   0    0    0   0   0   0   0   0  ...   0   0   0   0   1   \n",
       "43        0   0   0    0    0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "44        0   0   0    0    0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "45        0   0   0    0    0   0   0   0   0   0  ...   0   0   0   0   1   \n",
       "\n",
       "predict  40  41  42  43  44  \n",
       "labels                       \n",
       "0         0   0   0   0   0  \n",
       "1         0   0   0   0   0  \n",
       "2         0   0   0   0   0  \n",
       "3         0   0   0   0   0  \n",
       "4         0   0   0   1   0  \n",
       "5         0   0   0   0   0  \n",
       "6         0   0   0   0   0  \n",
       "7         0   0   0   0   0  \n",
       "8         0   1   0   0   0  \n",
       "9         0   0   0   0   0  \n",
       "10        0   0   0   0   0  \n",
       "11        0   0   0   0   0  \n",
       "12        0   0   0   0   0  \n",
       "13        0   0   0   0   0  \n",
       "14        0   0   0   0   0  \n",
       "15        0   0   0   0   0  \n",
       "16        0   0   0   0   0  \n",
       "17        0   0   0   0   0  \n",
       "18        0   0   0   0   0  \n",
       "19        0   0   0   0   0  \n",
       "20        0   0   0   0   0  \n",
       "21        0   0   0   0   0  \n",
       "22        0   0   0   0   0  \n",
       "23        0   0   0   0   0  \n",
       "24        0   0   0   0   0  \n",
       "25        0   0   0   0   0  \n",
       "26        0   0   0   0   0  \n",
       "27        0   0   0   0   0  \n",
       "28        0   0   0   0   0  \n",
       "29        0   0   0   0   0  \n",
       "30        0   0   0   0   0  \n",
       "31        0   0   0   0   0  \n",
       "32        0   0   0   0   0  \n",
       "33        0   0   0   0   0  \n",
       "34        0   0   0   0   0  \n",
       "35        0   0   0   0   0  \n",
       "36        0   0   0   0   0  \n",
       "37        0   0   0   0   0  \n",
       "38        0   0   0   0   0  \n",
       "39        0   0   0   0   0  \n",
       "40        3   0   0   0   0  \n",
       "41        0   0   0   0   0  \n",
       "42        0   0   1   0   0  \n",
       "43        0   0   0   6   0  \n",
       "44        0   0   0   0   4  \n",
       "45        0   0   0   0   0  \n",
       "\n",
       "[46 rows x 40 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.crosstab(test_labels,prediction,\n",
    "           rownames=['labels'],colnames=['predict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.58      0.70        12\n",
      "           1       0.64      0.83      0.72       105\n",
      "           2       0.76      0.65      0.70        20\n",
      "           3       0.89      0.94      0.92       813\n",
      "           4       0.84      0.87      0.85       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.66      0.66      0.66        38\n",
      "           9       0.86      0.72      0.78        25\n",
      "          10       0.89      0.80      0.84        30\n",
      "          11       0.62      0.70      0.66        83\n",
      "          12       0.25      0.08      0.12        13\n",
      "          13       0.60      0.65      0.62        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.50      0.11      0.18         9\n",
      "          16       0.66      0.76      0.71        99\n",
      "          17       0.80      0.33      0.47        12\n",
      "          18       0.73      0.55      0.63        20\n",
      "          19       0.66      0.65      0.65       133\n",
      "          20       0.47      0.66      0.55        70\n",
      "          21       0.64      0.67      0.65        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.40      0.17      0.24        12\n",
      "          24       0.50      0.21      0.30        19\n",
      "          25       0.87      0.65      0.74        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.20      0.10      0.13        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.80      0.33      0.47        12\n",
      "          31       0.50      0.46      0.48        13\n",
      "          32       1.00      0.60      0.75        10\n",
      "          33       1.00      0.60      0.75         5\n",
      "          34       0.80      0.57      0.67         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.50      0.27      0.35        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.30      0.46        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       0.86      1.00      0.92         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.79      2246\n",
      "   macro avg       0.60      0.44      0.48      2246\n",
      "weighted avg       0.78      0.79      0.77      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our approach reaches an accuracy of ~78%. With a balanced binary classification problem, the accuracy reached by a purely random classifier \n",
    "would be 50%, but in our case it is closer to 19%, so our results seem pretty good, at least when compared to a random baseline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种方法可以得到约 80% 的精度。对于平衡的二分类问题，完全随机的分类器能够得到\n",
    "50% 的精度。但在这个例子中，完全随机的精度约为 19%，所以上述结果相当不错，至少和随\n",
    "机的基准比起来还不错。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18833481745325023"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "test_labels_copy = copy.copy(test_labels)\n",
    "np.random.shuffle(test_labels_copy)\n",
    "float(np.sum(np.array(test_labels) == np.array(test_labels_copy))) / len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating predictions on new data\n",
    "\n",
    "We can verify that the `predict` method of our model instance returns a probability distribution over all 46 topics. Let's generate topic \n",
    "predictions for all of the test data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5.5　在新数据上生成预测结果  \n",
    "你可以验证，模型实例的 predict 方法返回了在 46 个主题上的概率分布。我们对所有测\n",
    "试数据生成主题预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry in `predictions` is a vector of length 46:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predictions 中的每个元素都是长度为 46 的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46,)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0179224e-05, 7.2923409e-05, 2.6935447e-06, 9.5992327e-01,\n",
       "       2.9007468e-02, 2.2002352e-05, 1.3775090e-05, 2.7002408e-05,\n",
       "       1.9644044e-04, 3.6517065e-06, 5.1312009e-06, 3.1052076e-04,\n",
       "       8.5252641e-06, 9.7586584e-05, 2.4325564e-06, 4.5549405e-06,\n",
       "       4.2440745e-04, 5.7505516e-05, 1.0173246e-05, 2.9807634e-04,\n",
       "       7.3171202e-03, 1.0017296e-04, 1.2999965e-06, 4.1331953e-04,\n",
       "       1.0601912e-05, 6.2084932e-06, 1.1321790e-05, 6.6130674e-06,\n",
       "       1.2790713e-05, 1.3053448e-04, 8.0497841e-05, 2.7144468e-05,\n",
       "       3.0110066e-05, 3.2231426e-06, 7.8761223e-05, 3.3940632e-06,\n",
       "       8.5185398e-04, 1.8946115e-05, 5.9394795e-05, 2.9271914e-04,\n",
       "       1.2807087e-06, 1.0393482e-05, 2.4580170e-06, 2.1605347e-05,\n",
       "       2.7052258e-07, 9.5589212e-06], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0] # 9.5992327e-01,这一项几率=0.95，编码位置为3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients in this vector sum to 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个向量的所有元素总和为 1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The largest entry is the predicted class, i.e. the class with the highest probability:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最大的元素就是预测类别，即概率最大的类别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A different way to handle the labels and the loss\n",
    "\n",
    "We mentioned earlier that another way to encode the labels would be to cast them as an integer tensor, like such:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5.6　处理标签和损失的另一种方法  \n",
    "前面提到了另一种编码标签的方法，就是将其转换为整数张量，如下所示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  4,  3, ..., 25,  3, 25], dtype=int64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The only thing it would change is the choice of the loss function. Our previous loss, `categorical_crossentropy`, expects the labels to \n",
    "follow a categorical encoding. With integer labels, we should use `sparse_categorical_crossentropy`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于这种编码方法，唯一需要改变的是损失函数的选择。对于代码清单 3-21 使用的损失\n",
    "函数 categorical_crossentropy，标签应该遵循分类编码。对于整数标签，你应该使用\n",
    "sparse_categorical_crossentropy。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new loss function is still mathematically the same as `categorical_crossentropy`; it just has a different interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个新的损失函数在数学上与 categorical_crossentropy 完全相同，二者只是接口不同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On the importance of having sufficiently large intermediate layers\n",
    "\n",
    "\n",
    "We mentioned earlier that since our final outputs were 46-dimensional, we should avoid intermediate layers with much less than 46 hidden \n",
    "units. Now let's try to see what happens when we introduce an information bottleneck by having intermediate layers significantly less than \n",
    "46-dimensional, e.g. 4-dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5.7　中间层维度足够大的重要性  \n",
    "前面提到，最终输出是 46 维的，因此中间层的隐藏单元个数不应该比 46 小太多。现在来\n",
    "看一下，如果中间层的维度远远小于 46（比如 4 维），造成了信息瓶颈，那么会发生什么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 3.3600 - accuracy: 0.2463 - val_loss: 2.1816 - val_accuracy: 0.5750\n",
      "Epoch 2/20\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.9009 - accuracy: 0.5953 - val_loss: 1.5774 - val_accuracy: 0.6150\n",
      "Epoch 3/20\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.4233 - accuracy: 0.6371 - val_loss: 1.4526 - val_accuracy: 0.6390\n",
      "Epoch 4/20\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 1.2393 - accuracy: 0.6567 - val_loss: 1.3589 - val_accuracy: 0.6680\n",
      "Epoch 5/20\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 1.0528 - accuracy: 0.7326 - val_loss: 1.3195 - val_accuracy: 0.6990\n",
      "Epoch 6/20\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.9401 - accuracy: 0.7612 - val_loss: 1.3165 - val_accuracy: 0.7060\n",
      "Epoch 7/20\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.8729 - accuracy: 0.7770 - val_loss: 1.3131 - val_accuracy: 0.7140\n",
      "Epoch 8/20\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.7804 - accuracy: 0.7953 - val_loss: 1.3155 - val_accuracy: 0.7130\n",
      "Epoch 9/20\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.7345 - accuracy: 0.8098 - val_loss: 1.3574 - val_accuracy: 0.7120\n",
      "Epoch 10/20\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.6658 - accuracy: 0.8276 - val_loss: 1.3846 - val_accuracy: 0.7250\n",
      "Epoch 11/20\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.6279 - accuracy: 0.8355 - val_loss: 1.4246 - val_accuracy: 0.7260\n",
      "Epoch 12/20\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.5786 - accuracy: 0.8420 - val_loss: 1.4828 - val_accuracy: 0.7280\n",
      "Epoch 13/20\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.5634 - accuracy: 0.8437 - val_loss: 1.5172 - val_accuracy: 0.7120\n",
      "Epoch 14/20\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.4958 - accuracy: 0.8641 - val_loss: 1.5687 - val_accuracy: 0.7130\n",
      "Epoch 15/20\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4915 - accuracy: 0.8680 - val_loss: 1.6521 - val_accuracy: 0.7180\n",
      "Epoch 16/20\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.4823 - accuracy: 0.8747 - val_loss: 1.6738 - val_accuracy: 0.7190\n",
      "Epoch 17/20\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.4455 - accuracy: 0.8803 - val_loss: 1.7553 - val_accuracy: 0.7150\n",
      "Epoch 18/20\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.4307 - accuracy: 0.8835 - val_loss: 1.7600 - val_accuracy: 0.7210\n",
      "Epoch 19/20\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.4115 - accuracy: 0.8836 - val_loss: 1.8090 - val_accuracy: 0.7240\n",
      "Epoch 20/20\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.3848 - accuracy: 0.8881 - val_loss: 1.8335 - val_accuracy: 0.7140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19fa176a700>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(4, activation='relu'))# 信息接收瓶颈，神经元太少了\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          validation_data=(x_val, y_val))\n",
    "#loss肉眼可见的非常大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our network now seems to peak at ~71% test accuracy, a 8% absolute drop. This drop is mostly due to the fact that we are now trying to \n",
    "compress a lot of information (enough information to recover the separation hyperplanes of 46 classes) into an intermediate space that is \n",
    "too low-dimensional. The network is able to cram _most_ of the necessary information into these 8-dimensional representations, but not all \n",
    "of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在网络的验证精度最大约为 71%，比前面下降了 8%。导致这一下降的主要原因在于，你\n",
    "试图将大量信息（这些信息足够恢复 46 个类别的分割超平面）压缩到维度很小的中间空间。网\n",
    "络能够将大部分必要信息塞入这个四维表示中，但并不是全部信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further experiments\n",
    "\n",
    "* Try using larger or smaller layers: 32 units, 128 units...\n",
    "* We were using two hidden layers. Now try to use a single hidden layer, or three hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5.8　进一步的实验  \n",
    "*  尝试使用更多或更少的隐藏单元，比如 32 个、128 个等。\n",
    "*  前面使用了两个隐藏层，现在尝试使用一个或三个隐藏层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "\n",
    "\n",
    "Here's what you should take away from this example:\n",
    "\n",
    "* If you are trying to classify data points between N classes, your network should end with a `Dense` layer of size N.\n",
    "* In a single-label, multi-class classification problem, your network should end with a `softmax` activation, so that it will output a \n",
    "probability distribution over the N output classes.\n",
    "* _Categorical crossentropy_ is almost always the loss function you should use for such problems. It minimizes the distance between the \n",
    "probability distributions output by the network, and the true distribution of the targets.\n",
    "* There are two ways to handle labels in multi-class classification:\n",
    "    ** Encoding the labels via \"categorical encoding\" (also known as \"one-hot encoding\") and using `categorical_crossentropy` as your loss \n",
    "function.\n",
    "    ** Encoding the labels as integers and using the `sparse_categorical_crossentropy` loss function.\n",
    "* If you need to classify data into a large number of categories, then you should avoid creating information bottlenecks in your network by having \n",
    "intermediate layers that are too small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5.9　小结  \n",
    "\n",
    "下面是你应该从这个例子中学到的要点。  \n",
    "\n",
    "*  如果要对 N 个类别的数据点进行分类，网络的最后一层应该是大小为 N 的 Dense 层。\n",
    "*  对于单标签、多分类问题，网络的最后一层应该使用 softmax 激活，这样可以输出在 N\n",
    "个输出类别上的概率分布。\n",
    "*  这种问题的损失函数几乎总是应该使用分类交叉熵。它将网络输出的概率分布与目标的\n",
    "真实分布之间的距离最小化。\n",
    "*  处理多分类问题的标签有两种方法。\n",
    "*  通过分类编码（也叫 one-hot 编码）对标签进行编码，然后使用 categorical_\n",
    "crossentropy 作为损失函数。\n",
    "*  将标签编码为整数，然后使用 sparse_categorical_crossentropy 损失函数。\n",
    "*  如果你需要将数据划分到许多类别中，应该避免使用太小的中间层，以免在网络中造成\n",
    "信息瓶颈。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
