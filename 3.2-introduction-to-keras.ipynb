{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Keras-has-the-following-key-features\" data-toc-modified-id=\"Keras-has-the-following-key-features-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Keras has the following key features</a></span></li><li><span><a href=\"#Developing-with-keras:The-are-two-ways-to-define-a-model:\" data-toc-modified-id=\"Developing-with-keras:The-are-two-ways-to-define-a-model:-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Developing with keras:The are two ways to define a model:</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2　Keras简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras has the following key features\n",
    "+ It allows the same code to run seamlessly on CPU or GPU.\n",
    "+ It has a user-friendly API.\n",
    "+ It has built-in support for convolutional networks(for computer vision),recurrent networks(for sequence processing),and any combination of both.\n",
    "+ It supports arbitrary network architectures:multi-input models,layer sharing,model sharing,and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras 具有以下重要特性。\n",
    "*  相同的代码可以在 CPU 或 GPU 上无缝切换运行。\n",
    "*  具有用户友好的 API，便于快速开发深度学习模型的原型。\n",
    "*  内置支持卷积网络（用于计算机视觉）、循环网络（用于序列处理）以及二者的任意\n",
    "组合。\n",
    "*  支持任意网络架构：多输入或多输出模型、层共享、模型共享等。这也就是说，Keras\n",
    "能够构建任意深度学习模型，无论是生成式对抗网络还是神经图灵机。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing with keras:The are two ways to define a model:\n",
    "+ The typical Keras workflow looks just like that example:\n",
    ">+ Define your training data: input tensors and target tensors\n",
    ">+ Define a network of layers(or model)that maps your inputs to your targets\n",
    ">+ Configure the learing process by chossing a loss function,an optimizer,and some metrics to monitor\n",
    ">+ Iterate on your traing data by calling the fit() method of your model.\n",
    "+ The Sequential class(only for linear stacks of layers,which is the most common network architecture by far)\n",
    "+ The functional API(for directed acyclic graphs of layers,which lets you build completely arbitrary architectures)\n",
    "+ With the functional API,you're manipulating the data tensors that the model processes and applying layers to this tensor as if they were functions.\n",
    "\n",
    "directed acyclic graphs : 有向无环图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (1) 定义训练数据：输入张量和目标张量。\n",
    "* (2) 定义层组成的网络（或模型），将输入映射到目标。\n",
    "* (3) 配置学习过程：选择损失函数、优化器和需要监控的指标。\n",
    "* (4) 调用模型的 fit 方法在训练数据上进行迭代。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Sequential class 定义训练数据\n",
    "from keras import models #模型\n",
    "from keras import layers #层\n",
    "\n",
    "#Sequential()方法是一个容器，描述了神经网络的网络结构，在Sequential()的输入参数中描述从输入层到输出层的网络结构\n",
    "#实际上是用来搭建网络的东西\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32,activation='relu',input_shape=(784,)))\n",
    "model.add(layers.Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.zhihu.com/question/29021768/answer/43488153  \n",
    "\n",
    "第一个问题：为什么引入非线性激励函数？如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层输出都是上层输入的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了。正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络就有意义了（不再是输入的线性组合，可以逼近任意函数）。最早的想法是sigmoid函数或者tanh函数，输出有界，很容易充当下一层输入（以及一些人的生物解释balabala）。  \n",
    "\n",
    "第二个问题：为什么引入Relu呢？第一，采用sigmoid等函数，算激活函数时（指数运算），计算量大，反向传播求误差梯度时，求导涉及除法，计算量相对大，而采用Relu激活函数，整个过程的计算量节省很多。第二，对于深层网络，sigmoid函数反向传播时，很容易就会出现梯度消失的情况（在sigmoid接近饱和区时，变换太缓慢，导数趋于0，这种情况会造成信息丢失，参见 @Haofeng Li  答案的第三点），从而无法完成深层网络的训练。  \n",
    "\n",
    "第三，Relu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生（以及一些人的生物解释balabala）。  \n",
    "\n",
    "relu函数表达式：$y=max(0,x)$  \n",
    "\n",
    "Softmax- 用于多分类神经网络的输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_shape：即张量的形状，从前往后对应由外向内的维度  \n",
    "32： 第一个隐藏层需要32个神经元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The functional API: 说明上面Sequential也可以使用API的方式写\n",
    "input_tensor = layers.Input(shape=(784,)) #输入a，输出input_tensor\n",
    "x = layers.Dense(32,activation='relu')(input_tensor) #输入input_tensor,输出x\n",
    "output_tensor = layers.Dense(10,activation='softmax')(x) #x又是下一层的输入\n",
    "model = models.Model(inputs=input_tensor,outputs=output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.01),loss='mse',metrics=['accuracy'])\n",
    "#loss选用标准差 优化器optimizers  RMSprop优化器：lr学习效率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras-cn.readthedocs.io/en/latest/other/optimizers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras model.compile(loss='目标函数 ', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-cd7959e4217c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#input_tensor：输入数据，target_tensor：输出数据，batch_size：\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#进行梯度下降时每次丢进去的数据数量，epochs：数据进行10次\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'target_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(input_tensor, target_tensor, batch_size=128, epochs=10)\n",
    "#input_tensor：输入数据，target_tensor：输出数据，batch_size：\n",
    "#进行梯度下降时每次丢进去的数据数量，epochs：数据进行10次"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里运行不成功，书上代码有误"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
